{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Denoising task",
      "provenance": [],
      "collapsed_sections": [
        "qc7BJnQF8zam"
      ],
      "toc_visible": true,
      "mount_file_id": "16HMTezUDuScumdwB4uDvVlxs9gKpT6uO",
      "authorship_tag": "ABX9TyPQ3KrbuG64xzSdntjx+3WW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamba124/just_for_fun/blob/classifier-in-autoenc/Denoising_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hyWYiPqLdez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Из кераса загружаем необходимые слои для нейросети\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras import backend as K # подтягиваем базовые керасовские функции\n",
        "from tensorflow.keras.optimizers import Adam # загружаем выбранный оптимизатор\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса\n",
        "from google.colab import files # модуль для загрузки файлов в colab\n",
        "import matplotlib.pyplot as plt # из библиотеки для визуализации данных возьмём интерфейс для построения графиков простых функций\n",
        "from tensorflow.keras.preprocessing import image # модуль для отрисовки изображения\n",
        "import numpy as np # библиотека для работы с массивами данных\n",
        "import librosa #Параметризация аудио\n",
        "import os \n",
        "\n",
        "import pandas as pd # библиотека для анализа и обработки данных\n",
        "from PIL import Image # модуль для отрисовки изображения\n",
        "from sklearn.model_selection import train_test_split # модуль для разбивки выборки на тренировочную/тестовую\n",
        "from sklearn.preprocessing import StandardScaler # модуль для стандартизации данных\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model #из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.optimizers import RMSprop #из кераса загружаем выбранный оптимизатор"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc7BJnQF8zam",
        "colab_type": "text"
      },
      "source": [
        "#Автокодировщик"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZAiEuYx899D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseAutoencoder(): # зададим функцию создания базового автокодировщика\n",
        "    img_input = Input((28,28,1)) # задаём входные размеры\n",
        "\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(img_input) # входные данные передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # затем пропускаем через слой нормализации данных \n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # далее снова слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # и еще слой нормализации данных\n",
        "    x = MaxPooling2D()(x) # передаём на слой подвыборки, снижающий размерность поступивших на него данных\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) # передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # пропускаем через слой нормализации данных \n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)  # далее снова слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # и еще слой нормализации данных\n",
        "    x = MaxPooling2D()(x) # передаём на слой подвыборки\n",
        "    # Изображение ужали до 7*7\n",
        "\n",
        "    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) # слой разжимает данные(с 7*7 на 14*14)\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) # передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) # ещё слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    # Сжатие MaxPooling2D не применяем\n",
        "\n",
        "    x = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) # слой разжимает данные(с 14*14 на 28*28)\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # ещё слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "\n",
        "    # Финальный слой двумерной свертки, выдающий итоговое изображение\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = Model(img_input, x) # указываем модель, с оригинальным изображением на входе в сеть и сжатым-разжатым на выходе из сети\n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='mean_squared_error') # компилируем модель с оптимайзером Адам и среднеквадратичной ошибкой\n",
        "\n",
        "    return model # функция вернёт заданную модель"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHiC0JfNKHUH",
        "colab_type": "text"
      },
      "source": [
        "# AUDIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upG9nFePiCol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a1da729a-e29a-4125-c5d8-927c46c418c2"
      },
      "source": [
        "!pip install soundfile"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ZoO27AcEQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa # for mel-spectrogram estimation\n",
        "import soundfile # for opening .flac audio\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cmARWGSkhw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st= '/content/drive/My Drive/train/clean/47/47_122796_47-122796-0071 (1).npy'\n",
        "stop = st[-7:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g957MMEVcEQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_chunks(signal, length, step):\n",
        "  batch =[]\n",
        "  syms_len = len(signal)\n",
        "  index = 0\n",
        "  \n",
        "  while (index + length <= syms_len):\n",
        "    batch.append(signal[index:index+length])\n",
        "    index += step\n",
        "\n",
        "  return batch\n",
        "\n",
        "def prepare_chunks(audio,length, step):\n",
        "  batch_list = []\n",
        "  for item in audio:\n",
        "    batch_list.append(get_chunks(item, length, step))\n",
        "\n",
        "  return np.array(batch_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD6WaJitJFgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "21810110-8b8d-4204-d0d4-3c033bdbe2a1"
      },
      "source": [
        "directory = '/content/drive/My Drive/train/'\n",
        "folders = os.listdir(directory)\n",
        "clean_arrays = []\n",
        "noisy_arrays = []\n",
        "patch = os.listdir(directory+'clean/')\n",
        "\n",
        "for folder in patch[:200]:\n",
        "  for arr in os.listdir(directory+'clean/'+folder):\n",
        "    if arr[-7:]!=stop:\n",
        "      clean_arrays.append(np.load(directory+'clean/'+folder+'/'+arr))\n",
        "      noisy_arrays.append(np.load(directory+'noisy/'+folder+'/'+arr))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c0c66522e920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mclean_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'clean/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mnoisy_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'noisy/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mclean_arrays_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjkBQJaCw_oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_arrays_ = []\n",
        "noisy_arrays_ = []\n",
        "\n",
        "for i in range(len(clean_arrays[:-1])):\n",
        "  for j in range(len(clean_arrays[i])):\n",
        "    clean_arrays_.append(clean_arrays[i][j])\n",
        "    noisy_arrays_.append(noisy_arrays[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47SaACdAictt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 60 \n",
        "step = 50\n",
        "\n",
        "x_trainC = prepare_chunks(clean_arrays_, length, step)\n",
        "y_trainC = np.ones(x_trainC.shape)\n",
        "\n",
        "x_trainN = prepare_chunks(noisy_arrays_, length, step)\n",
        "y_trainN = np.zeros(x_trainN.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkeN5CipkRlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classificator(vector):\n",
        "  shape_ = (vector.shape[1],vector.shape[2])\n",
        "  inputs = layers.Input(shape = shape_)\n",
        "  hidden = layers.Conv1D(8, 3)(inputs)\n",
        "  hidden = layers.Activation('tanh')(hidden)\n",
        "\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  hidden = layers.Dense(56)(hidden)\n",
        "  hidden = layers.Activation('relu')(hidden)\n",
        "\n",
        "  hidden = layers.Dense(100)(hidden)\n",
        "  out = layers.Dense(1, activation = 'sigmoid')(hidden)\n",
        "  \n",
        "  model = Model(inputs = inputs, outputs = out)\n",
        "\n",
        "  return model\n",
        "\n",
        "def autoencoder(vector):\n",
        "  #result = model.predict(vector)\n",
        "  inputs_decision = layers.Input(shape = (1,))\n",
        "  inputs_vector = layers.Input(shape = (vector.shape[1],vector.shape[2]))\n",
        "  ##Block1##\n",
        "  hidden = layers.Conv1D(16, 3) (inputs_vector)\n",
        "  hidden = layers.Activation('tanh')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.Conv1D(16, 3)(hidden)\n",
        "  hidden = layers.Activation('relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.MaxPooling1D(hidden)\n",
        "\n",
        "  out_class = layers.Flatten()(hidden)\n",
        "\n",
        "  out_class = layers.Dense(56, activation = 'relu')(out_class)\n",
        "  out_class = layers.Dense(1, activation = 'sigmoid')(out_class)\n",
        "\n",
        "  ##Block2##\n",
        "  hidden = layers.Conv1D(64, 3) (hidden)\n",
        "  hidden = layers.Activation('tanh')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.Conv1D(64, 3)(hidden)\n",
        "  hidden = layers.Activation('relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.MaxPooling1D(hidden)\n",
        "\n",
        "  \n",
        "  ##Block3##\n",
        "  #hidden = Conv1D(128, 3) (hidden)\n",
        "  #hidden = Activation('tanh')\n",
        "  #hidden = BatchNormalization()(hidden)\n",
        "  #hidden = Conv1D(128, 3)(hidden)\n",
        "  #hidden = Activation('relu')\n",
        "  #hidden = BatchNormalization()(hidden)\n",
        "  #hidden = MaxPooling1D(hidden)  \n",
        "\n",
        "  ##UPBlock1##\n",
        "  hidden = layers.UpSampling1D(hidden)\n",
        "  hidden = layers.Conv1D(64, 3, activation= 'relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  \n",
        "  ##UPblock2\n",
        "  hidden = layers.UpSampling1D(hidden)\n",
        "  hidden = layers.Conv1D(16, 3, activation= 'relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  hidden = layers.Concatenate([out_class, hidden])\n",
        "  hidden = layers.Dense(128, activation='relu')(hidden)\n",
        "  out = layers.Dense(inputs_vector.size)(hidden)\n",
        "\n",
        "  modelD = Model(inputs=inputs_vector, outputs = [out, out_class])\n",
        "\n",
        "  return modelD\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDm1EtKtVhlv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "d803b3a3-929e-4e01-f683-3796740ddf8a"
      },
      "source": [
        "def shuffle(a, b):\n",
        "    p = np.random.permutation(len(a)) \n",
        "    return a[p], b[p] \n",
        "x_train = np.concatenate(x_trainC, x_trainN)\n",
        "y_train = np.concatenate(y_trainC, y_trainN)\n",
        "\n",
        "x_train_, y_train_ = shuffle(x_train, y_train)\n",
        "#x_val_, y_val_ = shuffle(x_val, y_val)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6ffea66c6d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_trainN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trainC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_trainC' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daZg86xp2D_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8bee1ac-a1fc-4254-a140-4cbd35c6c7ea"
      },
      "source": [
        "classificator = classificator(clean_arrays)\n",
        "classificator.compile(optimizer = RMSprop(), loss = 'mse', metrics = 'accuracy')\n",
        "classificator.fit(x_train_, y_train_, batch_size=100, epochs=20, validation_data=x_val_, y_val_)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJB_8QMVGLje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "denoise = autoencoder(clean_arrays_)\n",
        "denoise.compile(optimizer = RMSprop(), loss = 'mse', metrics = 'accuracy')\n",
        "denoise.fit(x_train_, [x_train_, y_train_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds_tXun7VYJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "denoised, classes = denoise.predict(x_train_[-300:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpYnPfBQZOpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def glue_chunks(signal_list):\n",
        "  signals = []\n",
        "  for signal in signal_list:\n",
        "    glued = []\n",
        "    for i in range(len(signal)):\n",
        "      if i == 0: ind = 0\n",
        "      else: ind = length - step\n",
        "      for j in range(ind,length):\n",
        "        glued.append(signal[i][j])\n",
        "    signals.append(np.array(glued))\n",
        "\n",
        "  signals = np.array(signals)\n",
        "\n",
        "denoised_glued = glue_chunks(denoised)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BSoAm6Cn93W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, record in enumerate(denoised_glued): \n",
        "  np.save('/final/' + i, record)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}