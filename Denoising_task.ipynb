{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Denoising task",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "16HMTezUDuScumdwB4uDvVlxs9gKpT6uO",
      "authorship_tag": "ABX9TyNIbg7c3SY1MsELV5yTLBmn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamba124/just_for_fun/blob/master/Denoising_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hyWYiPqLdez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Из кераса загружаем необходимые слои для нейросети\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras import backend as K # подтягиваем базовые керасовские функции\n",
        "import tensorflow.keras.optimizers as optimizers# загружаем выбранный оптимизатор\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса\n",
        "from google.colab import files # модуль для загрузки файлов в colab\n",
        "import matplotlib.pyplot as plt # из библиотеки для визуализации данных возьмём интерфейс для построения графиков простых функций\n",
        "from tensorflow.keras.preprocessing import image # модуль для отрисовки изображения\n",
        "import numpy as np # библиотека для работы с массивами данных\n",
        "import librosa #Параметризация аудио\n",
        "import os \n",
        "\n",
        "import pandas as pd # библиотека для анализа и обработки данных\n",
        "from PIL import Image # модуль для отрисовки изображения\n",
        "from sklearn.model_selection import train_test_split # модуль для разбивки выборки на тренировочную/тестовую\n",
        "from sklearn.preprocessing import StandardScaler # модуль для стандартизации данных\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model #из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.optimizers import RMSprop #из кераса загружаем выбранный оптимизатор"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc7BJnQF8zam",
        "colab_type": "text"
      },
      "source": [
        "#Автокодировщик"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZAiEuYx899D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseAutoencoder(): # зададим функцию создания базового автокодировщика\n",
        "    img_input = Input((28,28,1)) # задаём входные размеры\n",
        "\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(img_input) # входные данные передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # затем пропускаем через слой нормализации данных \n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # далее снова слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # и еще слой нормализации данных\n",
        "    x = MaxPooling2D()(x) # передаём на слой подвыборки, снижающий размерность поступивших на него данных\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) # передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # пропускаем через слой нормализации данных \n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)  # далее снова слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # и еще слой нормализации данных\n",
        "    x = MaxPooling2D()(x) # передаём на слой подвыборки\n",
        "    # Изображение ужали до 7*7\n",
        "\n",
        "    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) # слой разжимает данные(с 7*7 на 14*14)\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) # передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) # ещё слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    # Сжатие MaxPooling2D не применяем\n",
        "\n",
        "    x = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) # слой разжимает данные(с 14*14 на 28*28)\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # передаем на слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # ещё слой двумерной свёртки\n",
        "    x = BatchNormalization()(x) # слой нормализации данных\n",
        "\n",
        "    # Финальный слой двумерной свертки, выдающий итоговое изображение\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = Model(img_input, x) # указываем модель, с оригинальным изображением на входе в сеть и сжатым-разжатым на выходе из сети\n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='mean_squared_error') # компилируем модель с оптимайзером Адам и среднеквадратичной ошибкой\n",
        "\n",
        "    return model # функция вернёт заданную модель"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHiC0JfNKHUH",
        "colab_type": "text"
      },
      "source": [
        "# AUDIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upG9nFePiCol",
        "colab_type": "code",
        "outputId": "2567705e-4975-4f9b-b43a-2f3217589c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install soundfile"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ZoO27AcEQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa # for mel-spectrogram estimation\n",
        "import soundfile # for opening .flac audio\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cmARWGSkhw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st= '/content/drive/My Drive/train/clean/47/47_122796_47-122796-0071 (1).npy'\n",
        "stop = st[-7:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g957MMEVcEQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_chunks(signal, length, step):\n",
        "  batch =[]\n",
        "  syms_len = len(signal)\n",
        "  index = 0\n",
        "  \n",
        "  while (index + length <= syms_len):\n",
        "    batch.append(signal[index:index+length])\n",
        "    index += step\n",
        "\n",
        "  return batch\n",
        "\n",
        "def prepare_chunks(audio,length, step):\n",
        "  batch_list = []\n",
        "  #for item in audio:\n",
        "   # batch_list.append(get_chunks(item, length, step))\n",
        "  batch_list = get_chunks(audio, length, step)\n",
        "  return np.array(batch_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD6WaJitJFgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = '/content/drive/My Drive/train/'\n",
        "folders = os.listdir(directory)\n",
        "clean_arrays = []\n",
        "noisy_arrays = []\n",
        "patch = os.listdir(directory+'clean/')\n",
        "\n",
        "for folder in patch[:300]:\n",
        "  for arr in os.listdir(directory+'clean/'+folder):\n",
        "    if arr[-7:]!=stop:\n",
        "      clean_arrays.append(np.load(directory+'clean/'+folder+'/'+arr))\n",
        "      noisy_arrays.append(np.load(directory+'noisy/'+folder+'/'+arr))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjkBQJaCw_oL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "e864a804-4e16-476c-bd58-70307e7701c2"
      },
      "source": [
        "clean_arrays_ = []\n",
        "noisy_arrays_ = []\n",
        "\n",
        "for i in range(len(clean_arrays[:-1])):\n",
        "  for j in range(len(clean_arrays[i])):\n",
        "    clean_arrays_.append(clean_arrays[i][j])\n",
        "    noisy_arrays_.append(noisy_arrays[i][j])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-44d0cb6c58dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnoisy_arrays_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclean_arrays_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clean_arrays' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMLRJc_yqbEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('c.npy', clean_arrays_)\n",
        "np.save('n.npy', noisy_arrays_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OobRzf25qpwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('c1.npy', x_trainC)\n",
        "np.save('n1.npy',x_trainN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47SaACdAictt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 60 \n",
        "step = 30\n",
        "\n",
        "x_trainC = prepare_chunks(clean_arrays_, length, step)\n",
        "x_trainN = prepare_chunks(noisy_arrays_, length, step)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlLVZQ1nrgGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 60 \n",
        "step = 30\n",
        "x_trainC = np.load('c1.npy')\n",
        "x_trainN = np.load('n1.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA0p_rv_yKkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_trainC = np.ones((x_trainC.shape[0],1))\n",
        "y_trainN = np.zeros((x_trainN.shape[0],1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9sXs_o9djUj",
        "colab_type": "code",
        "outputId": "e9cc9c13-42f2-45f8-b7a2-21f84070b237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "x_trainC[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.3887 ,  0.2979 ,  0.4097 , ..., -0.8145 , -0.833  , -0.848  ],\n",
              "       [ 0.2964 ,  0.3025 ,  0.334  , ..., -0.8374 , -0.8276 , -0.8516 ],\n",
              "       [ 0.119  ,  0.2399 ,  0.1682 , ..., -0.851  , -0.845  , -0.8433 ],\n",
              "       ...,\n",
              "       [ 0.6104 ,  0.4626 ,  0.3252 , ...,  0.03114,  0.0225 , -0.01971],\n",
              "       [ 0.693  ,  0.7256 ,  0.854  , ..., -0.1829 , -0.2048 , -0.2715 ],\n",
              "       [ 0.6665 ,  0.947  ,  1.076  , ..., -0.2837 , -0.287  , -0.337  ]],\n",
              "      dtype=float16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDm1EtKtVhlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle(a, b):\n",
        "    p = np.random.permutation(len(a)) \n",
        "    return a[p], b[p] \n",
        "x_train = np.concatenate([x_trainC, x_trainN], axis = 0)\n",
        "y_train = np.concatenate([y_trainC, y_trainN], axis = 0)\n",
        "\n",
        "x_train_, y_train_ = shuffle(x_train, y_train)\n",
        "#x_val_, y_val_ = shuffle(x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkeN5CipkRlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classificator(vector):\n",
        "  shape_ = (vector.shape[1],vector.shape[2])\n",
        "  inputs = layers.Input(shape = shape_)\n",
        "  hidden = layers.Conv1D(8, 3)(inputs)\n",
        "  hidden = layers.Activation('tanh')(hidden)\n",
        "\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  hidden = layers.Dense(56)(hidden)\n",
        "  hidden = layers.Activation('relu')(hidden)\n",
        "\n",
        "  hidden = layers.Dense(100)(hidden)\n",
        "  out = layers.Dense(1, activation = 'sigmoid')(hidden)\n",
        "  \n",
        "  model = Model(inputs = inputs, outputs = out)\n",
        "\n",
        "  return model\n",
        "\n",
        "def autoencoder(vector):\n",
        "\n",
        "  inputs_vector = layers.Input(shape = (vector.shape[1],vector.shape[2]))\n",
        "\n",
        "  out_class = layers.BatchNormalization()(inputs_vector)\n",
        "  out_class= layers.Conv1D(64, 3, padding = 'same')(out_class)\n",
        "  out_class = layers.Flatten()(out_class)\n",
        "  out_class = layers.Dense(100, activation='elu')(out_class)\n",
        "  out_class = layers.Dense(56, activation = 'relu')(out_class)\n",
        "  out_class = layers.Dense(1, activation = 'sigmoid')(out_class)\n",
        "  \n",
        "  ##Block1##\n",
        "  hidden = layers.Conv1D(2, 3, padding = 'same') (inputs_vector)\n",
        "  hidden = layers.Activation('tanh')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "\n",
        "  hidden = layers.Conv1D(2, 3, padding = 'same')(hidden)\n",
        "  hidden = layers.Activation('relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "\n",
        "  hidden = layers.MaxPooling1D(3)(hidden)\n",
        "\n",
        "  \n",
        "  ##Block2##\n",
        "  hidden = layers.Conv1D(4, 3, padding = 'same') (hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.Conv1D(4, 3, padding = 'same', strides = 2)(hidden)\n",
        "  hidden = layers.Activation('relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.MaxPooling1D(3)(hidden)\n",
        "  \n",
        "  ##Block3##\n",
        "  hidden = layers.Conv1D(128, 3, padding = 'same') (hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.MaxPooling1D(3)(hidden)  \n",
        "  \n",
        "  ##UPBlock1##\n",
        "  hidden = layers.UpSampling1D(3)(hidden)\n",
        "  hidden = layers.Conv1D(64, 3, padding = 'same', activation= 'relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.Multiply()([out_class, hidden])\n",
        "\n",
        "  ##UPBlock2##\n",
        "  hidden = layers.UpSampling1D(4)(hidden)\n",
        "  hidden = layers.Conv1D(64, 3, padding = 'same', activation= 'relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.Multiply()([out_class, hidden])\n",
        "\n",
        "  ##UPblock3##\n",
        "  hidden = layers.UpSampling1D(5)(hidden)\n",
        "  hidden = layers.Conv1D(64, 3, padding = 'same',  activation= 'relu')(hidden)\n",
        "  hidden = layers.BatchNormalization()(hidden)\n",
        "  hidden = layers.Multiply()([out_class, hidden])\n",
        "\n",
        "  out = layers.Conv1D(80, 3, padding = 'same',  activation= 'tanh')(hidden)\n",
        "\n",
        "  modelD = Model(inputs=inputs_vector, outputs = [out, out_class])\n",
        "  #modelD.summary()\n",
        "\n",
        "  return modelD\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJB_8QMVGLje",
        "colab_type": "code",
        "outputId": "fcc32e9d-3638-40f5-9e3e-75526f3f8fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "denoise = autoencoder(x_train_)\n",
        "denoise.compile(optimizer = RMSprop(), loss = ['mae','binary_crossentropy'], metrics = ['mse','accuracy'])\n",
        "denoise.fit(x_train_, [x_train_, y_train_], validation_split = 0.1, batch_size=500, epochs = 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "416/416 [==============================] - 41s 99ms/step - loss: 0.5687 - conv1d_155_loss: 0.2624 - dense_65_loss: 0.3063 - conv1d_155_mse: 0.1150 - conv1d_155_accuracy: 0.0955 - dense_65_mse: 0.0890 - dense_65_accuracy: 0.8802 - val_loss: 0.4578 - val_conv1d_155_loss: 0.2421 - val_dense_65_loss: 0.2157 - val_conv1d_155_mse: 0.0947 - val_conv1d_155_accuracy: 0.0965 - val_dense_65_mse: 0.0642 - val_dense_65_accuracy: 0.9121\n",
            "Epoch 2/50\n",
            "416/416 [==============================] - 40s 97ms/step - loss: 0.4135 - conv1d_155_loss: 0.2143 - dense_65_loss: 0.1992 - conv1d_155_mse: 0.0776 - conv1d_155_accuracy: 0.1125 - dense_65_mse: 0.0572 - dense_65_accuracy: 0.9236 - val_loss: 0.3949 - val_conv1d_155_loss: 0.2123 - val_dense_65_loss: 0.1826 - val_conv1d_155_mse: 0.0765 - val_conv1d_155_accuracy: 0.1225 - val_dense_65_mse: 0.0528 - val_dense_65_accuracy: 0.9285\n",
            "Epoch 3/50\n",
            "416/416 [==============================] - 40s 95ms/step - loss: 0.3724 - conv1d_155_loss: 0.2071 - dense_65_loss: 0.1653 - conv1d_155_mse: 0.0726 - conv1d_155_accuracy: 0.1177 - dense_65_mse: 0.0472 - dense_65_accuracy: 0.9371 - val_loss: 0.3865 - val_conv1d_155_loss: 0.2040 - val_dense_65_loss: 0.1825 - val_conv1d_155_mse: 0.0708 - val_conv1d_155_accuracy: 0.1199 - val_dense_65_mse: 0.0515 - val_dense_65_accuracy: 0.9315\n",
            "Epoch 4/50\n",
            "416/416 [==============================] - 40s 97ms/step - loss: 0.3502 - conv1d_155_loss: 0.2042 - dense_65_loss: 0.1461 - conv1d_155_mse: 0.0706 - conv1d_155_accuracy: 0.1207 - dense_65_mse: 0.0415 - dense_65_accuracy: 0.9448 - val_loss: 0.3669 - val_conv1d_155_loss: 0.2042 - val_dense_65_loss: 0.1627 - val_conv1d_155_mse: 0.0705 - val_conv1d_155_accuracy: 0.1190 - val_dense_65_mse: 0.0465 - val_dense_65_accuracy: 0.9381\n",
            "Epoch 5/50\n",
            "416/416 [==============================] - 40s 97ms/step - loss: 0.3314 - conv1d_155_loss: 0.2027 - dense_65_loss: 0.1287 - conv1d_155_mse: 0.0696 - conv1d_155_accuracy: 0.1225 - dense_65_mse: 0.0362 - dense_65_accuracy: 0.9525 - val_loss: 0.3748 - val_conv1d_155_loss: 0.2018 - val_dense_65_loss: 0.1731 - val_conv1d_155_mse: 0.0692 - val_conv1d_155_accuracy: 0.1227 - val_dense_65_mse: 0.0473 - val_dense_65_accuracy: 0.9384\n",
            "Epoch 6/50\n",
            "255/416 [=================>............] - ETA: 14s - loss: 0.3159 - conv1d_155_loss: 0.2019 - dense_65_loss: 0.1140 - conv1d_155_mse: 0.0690 - conv1d_155_accuracy: 0.1236 - dense_65_mse: 0.0319 - dense_65_accuracy: 0.9580"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daZg86xp2D_s",
        "colab_type": "code",
        "outputId": "4770422b-9e30-44ca-ce74-9d135cc801fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#classificator = classificator(x_train_)\n",
        "classificator.compile(optimizer = RMSprop(), loss = 'binary_crossentropy', metrics = 'accuracy')\n",
        "classificator.fit(x_train_, y_train_, batch_size=500, epochs=20, validation_splt = 0.1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "463/463 [==============================] - 6s 13ms/step - loss: 0.3255 - accuracy: 0.8609\n",
            "Epoch 2/20\n",
            "425/463 [==========================>...] - ETA: 0s - loss: 0.2520 - accuracy: 0.8977"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-b5f2159c7cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassificator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassificator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_splt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OVk8Ec41-b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds_tXun7VYJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "denoised, classes = denoise.predict(x_train_[-300:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpYnPfBQZOpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def glue_chunks(signal_list):\n",
        "  signals = []\n",
        "  for signal in signal_list:\n",
        "    glued = []\n",
        "    for i in range(len(signal)):\n",
        "      if i == 0: ind = 0\n",
        "      else: ind = length - step\n",
        "      for j in range(ind,length):\n",
        "        glued.append(signal[i][j])\n",
        "    signals.append(np.array(glued))\n",
        "\n",
        "  signals = np.array(signals)\n",
        "\n",
        "denoised_glued = glue_chunks(denoised)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BSoAm6Cn93W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, record in enumerate(denoised_glued): \n",
        "  np.save('/final/' + i, record)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}